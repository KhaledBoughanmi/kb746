<html>
<head>
<title>Khaled Boughanmi</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114722060-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114722060-1');
</script>



<LINK rel="stylesheet" type="text/css" name="main" href="styles.css">
<!--###########################
    Expand/Collapse JS and CSS code for use
    in the HEAD section of your document.
    Written by Dick Ervasti. Learn More at:
    http://dickervasti.com/wiki-style-text-expand-collapse-no-jquery.htm
    ##################################-->
    <script type="text/javascript">
    <!--
        function expand_collapse(id) {
           var e = document.getElementById(id);
           var f = document.getElementById(id+"_collapse");
           if(e.style.display == 'none'){
              e.style.display = 'block';
              f.innerHTML = 'Hide Abstract';
           }
           else {
              e.style.display = 'none';
              f.innerHTML = 'Show Abstract';
           }
        }
    //-->
    </script>
    <style type="text/css">
    .arrows{text-decoration:none;color:silver;}
    </style>
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-114722060-1', 'auto');
	  ga('send', 'pageview');

	</script>
</head>


  <menubar>
    <div id="menu-container">
      <ul>
	<li style="width:50%; text-align:left;">
	  <a href="./index.html"><b><br><br>Home</b></a>
	</li>

	<li style="width:50%; text-align:left;">
	  <a href="./research.html"><b><br><br>Research</b></a>
	</li>

	<li style="width:50%; text-align:left;">
	  <a href="cv.pdf"><b><br><br>Curriculum Vitae</b></a>
	</li>

     </ul>
    </div>
  </menubar>


</td>
</table>
<table align="center" width=800>
<tr>
<td>

<h1>Khaled Boughanmi</h1>
Marketing <br>
Cornell University, Johnson Graduate School of Management
<br>
<br>
<ul>

<h2>Publications and Accepted Papers</h2>
<ul>

<li><a href="https://www.researchgate.net/publication/326252464_Randomized_algorithms_for_lexicographic_inference" target="_blank">Randomized Algorithms for Lexicographic Inference</a> (2017),<br>
Rajeev Kohli, <b>Khaled Boughanmi</b> and Vikram Kohli.<br>
Forthcoming at <em>Operations Research</em>.<br>

[<a href="#abstract" id="1_collapse" onclick="expand_collapse('1');">Show Abstract</a>]
[<a href="https://www.researchgate.net/publication/314078578_Randomized_algorithms_for_lexicographic_inference" target="_blank">Paper</a>]
[<a href="rand_alg_lex.pdf" target="_blank">Slides</a>]</a>  <br>

<div id="1" style="display:none;">
<blockquote>
The inference of a lexicographic rule from paired comparisons, ranking or choice data is a discrete optimization problem that generalizes the linear ordering problem.
We develop an approach to its solution using randomized algorithms.
First, we show that maximizing the expected value of a randomized solution is equivalent to solving the lexicographic inference problem.
As a result, the discrete problem is transformed into a continuous and unconstrained nonlinear program that can be solved, possibly only to a local optimum, using standard nonlinear optimization methods.
Second, we show that a maximum likelihood procedure, which runs in polynomial time, can be used to implement the randomized algorithm.
The maximum likelihood value determines a lower bound on the performance ratio of the randomized algorithm.
We employ the proposed approach to infer lexicographic rules for individuals using data from a choice experiment for electronic tablets.
These rules obtain substantially better fit and predictions than a previously described greedy algorithm, a local-search algorithm, and multinomial logit and probit models.
</blockquote>
</div>
<br>

<h2>Manuscripts under Review and Working Papers</h2>
<li><a href=music_dynamics.pdf target="_blank"> Dynamics of Musical Success: A Bayesian Nonparametric Approach</a> (2017),<br> <em> </em>
<b>Khaled Boughanmi</b> and Asim Ansari.<br>
Invited for a revision at <em>Journal of Marketing Research.</em><br>
<em></em>
This project won the Deming Center Doctoral Fellowship award.
<br>
[<a href="#abstract" id="0_collapse" onclick="expand_collapse('0');">Show Abstract</a>]
[<a href= music_dynamics.pdf target="_blank">Paper</a>]
[<a href= dynamics_nonparametrics.py target="_blank">Python Code</a>]
[<a href= Utils.py target="_blank">Tools</a>]

<div id="0" style="display:none;">
<blockquote>
We study the dynamics of musical success over the past five decades.
We develop a novel nonparametric Bayesian modeling framework that combines data of different modalities (e.g. metadata, acoustic  and textual data) to infer the correlates of album success.
We then show how artists, music platforms and label houses  can use the model estimates to compile new music albums and playlists.
Our empirical investigation uses a dataset spanning 54 years of popular American music, which we collected using different online sources. The modeling framework integrates different types of nonparametrics.
One component uses a supervised hierarchical Dirichlet process to summarize the perceptual information in crowd-sourced
textual tags; another time-varying component uses dynamic penalized splines to capture how
different acoustical features of music have shaped album success over the years.
Our results  illuminate the broad patterns in the rise and decline of different
musical genres, and in the emergence of new forms of music.
They also characterize how various subjective and objective acoustic measures have
waxed and waned in importance over the years. We uncover a number of themes that
categorize albums in terms of sub-genres, consumption contexts, emotions, nostalgia
and other aspects of the musical experience. We show how the parameters of our model
can be used to construct  music compilations and playlists that are likely to appeal
to listeners with different preferences and requirements. </blockquote>
</div>
<br>
<br>


<li><a href=" http://ssrn.com/abstract=3129468" target="_blank">Framing, Context and Value Averaging</a> (2016),<br>
<b>Khaled Boughanmi</b>, Kamel Jedidi and Rajeev Kohli.<br>
Invited for a revision at <em>Journal of Marketing Research.</em><br>
This project won the Luxury education award.<br>
[<a href="#abstract" id="2_collapse" onclick="expand_collapse('2');">Show Abstract</a>]
[<a href=" http://ssrn.com/abstract=3129468" target="_blank">Paper</a>]
[<a href="r_model.pdf" target="_blank">Slides</a>]</a>  <br>

<div id="2" style="display:none;">
<blockquote>
We introduce a multi-attribute model that generalizes the multinomial logit model by introducing one additional parameter.
 It captures framing and context effects, and allows violations of regularity, independence of irrelevant alternatives and order independence.
 Conceptually, the model considers the value of an alternative to be a generalized mean of the importance values associated with its attributes.
A single parameter determines the type of mean. Its value can change across decision frames and choice sets.
 A positive/negative parameter value  corresponds to a positive/negative evaluation frame, and a more negative (positive) parameter value reflects greater
 aversion to (tendency towards)  choosing extreme outcomes.  Limiting cases of the model correspond to lexicographic rules by which the value of an alternative
is determined solely by its best or worst attribute. We use data from published studies to illustrate how the model captures the effects of context and framing on choice.
We describe an application concerning digital cameras and discuss the implications of the model for product design, product positioning and demand forecasting.
</blockquote>
</div>

<br>

<li>The Impact of Fame on Artistic Production</a>,<br> <em> </em>
<b>Khaled Boughanmi, Olivier Toubia</b> and Asim Ansari.<br>
<br>




<li><a href="https://www.researchgate.net/publication/314078479_Linking_continuous_and_discrete_linear_ordering_problems" target="_blank">Solving Large Linear Ordering Problems</a> (2017),<br>
Rajeev Kohli, <b>Khaled Boughanmi</b> and Vikram Kohli.<br>
To be submitted to <em>Management Science</em>.<br>
[<a href="#abstract" id="3_collapse" onclick="expand_collapse('3');">Show Abstract</a>]
[<a href="https://www.researchgate.net/publication/314078479_Linking_continuous_and_discrete_linear_ordering_problems" target="_blank">Paper</a>]
[<a href="lin_ord.pdf" target="_blank">Slides</a>]</a>  <br>

<div id="3" style="display:none;">
<blockquote>
The linear ordering problem combines multiple rankings or paired comparisons  of alternatives into a single representative ranking.
It is used, for example, by Facebook to construct personalized newsfeeds and by Twitter to form while-you-were-away lists.
The standard formulation of the linear ordering problem is a computationally complex discrete optimization problem that uses paired-comparisons data.
We propose an alternative approach using a random-utility framework. This approach subsumes the discrete formulation as a special case; explicitly
models uncertainty and error in outcomes; distinguishes among paired comparison, ranking and choice data; and can be solved efficiently (in polynomial time)
using maximum likelihood upon assuming that the random utilities have independent extreme value distributions. The maximum likelihood solution can also be used
to implement an efficient randomized algorithm  that obtains an approximate solution to the discrete optimization version of the linear ordering problem.
As the likelihood value increases, a lower bound on the expected value of the randomized solution increases towards the optimal solution value of the discrete problem.
 The proposed approach is used for ranking up to two-thousand funny videos using paired-comparisons data collected by a research lab at YouTube.
The maximum likelihood solution is obtained quickly for large problems. It also closely approximates the optimal solution to the discrete linear ordering problem.
</blockquote>
</div>

<br>

<li>Adaptive Customization (2018)</a>,<br>
<em> </em>
Rajeev Kohli and <b>Khaled Boughanmi.</b> <br>
To be submitted to <em>Marketing Science</em>.
<br>
[<a href="#abstract" id="4_collapse" onclick="expand_collapse('4');">Show Abstract</a>]
[<a href="adapt_cust.pdf" target="_blank">Slides</a>]<br>
<br>

<div id="4" style="display:none;">
<blockquote>
E-commerce sites often allow users to narrow the available alternatives into increasingly smaller sets by
selecting a sequence of relevant features. After each feature is selected, we predict the sequence of features
a shopper is most likely to use next. The predictions can change after the selection of each additional
feature. We use these predictions to adaptively customize the display of alternatives and feature menus,
and to recommend products to shoppers. The proposed approach uses oine and/or online data to identify
latent consumer segments, and uses the information obtained when a shopper chooses a feature to update
his/her segment membership probabilities using Bayes' rule. The optimal predicted sequence is obtained by
weighting the segment-level sequence probabilities by the posterior membership probabilities. The predicted
sequences are required only for a few steps because the screening process typically terminates after four orfive steps. We illustrate the proposed approach using data on consumer choices for electronic tablets. </blockquote>
</div>


<li><a href=Fit_or_Hit.pdf target="_blank">Hit or Fit in Choice Models</a>,<br> <em> </em>
<b>Khaled Boughanmi</b>, Rajeev Kohli and Kamel Jedidi.<br>
[<a href="#abstract" id="5_collapse" onclick="expand_collapse('5');">Show Abstract</a>]
[<a href="Fit_or_Hit.pdf" target="_blank">Paper</a>]<br>

<div id="5" style="display:none;">
<blockquote>
The predictive validity of a choice model is often assessed by its hit rate.
We examine and illustrate conditions under which a choice model with a higher likelihood value may obtain a lower hit rate.
We also show that the solution obtained by maximizing a likelihood function can be different from the solution obtained by maximizing the hit rate.
The analysis and results suggest that the hit rate should not be overly emphasized when the objective is testing a theory and/or statistical inference.
But if the aim is prediction, then the expected hit rate can be maximized. </blockquote>
</div>
<br>

<li><a href=sheba.pdf target="_blank">Learning for Non Compensatory Models</a>,<br> <em> </em>
<b>Khaled Boughanmi</b>, Asim Ansari and Rajeev Kohli.<br>
<!--[<a href="#abstract" id="6_collapse" onclick="expand_collapse('6');">Show Abstract</a>]
[<a href="test" target="_blank">Paper</a>] -->
[<a href="sheba.pdf" target="_blank">Slides</a>]</a>  <br>

<div id="6" style="display:none;">
<blockquote>
The predictive validity of a choice model is often assessed by its hit rate.
We examine and illustrate conditions under which a choice model with a higher likelihood value may obtain a lower hit rate.
We also show that the solution obtained by maximizing a likelihood function can be different from the solution obtained by maximizing the hit rate.
The analysis and results suggest that the hit rate should not be overly emphasized when the objective is testing a theory and/or statistical inference.
But if the aim is prediction, then the expected hit rate can be maximized. </blockquote>
</div>
<br>
